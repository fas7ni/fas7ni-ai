{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e392b326-b3b0-4934-829c-f2e804997e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import timm  # For advanced vision transformers\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9210ae2-de2a-430e-be6c-75a479ba04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "batch_size = 16\n",
    "size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d2c0d6-b978-4cf3-929d-d3d788cf67a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ac900e-128d-4bd2-bf15-29f2a51d56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 1. Define the Prototypical Network\n",
    "# =============================\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        \"\"\"\n",
    "        This network uses an advanced Vision Transformer (DeiT-Small) from timm.\n",
    "        - It loads a pretrained model (which is under 250 MB in disk size).\n",
    "        - It freezes all parameters except the last two transformer blocks for fine-tuning.\n",
    "        - It adds an embedding head mapping the backbone output to a 128-dim embedding.\n",
    "        - It extracts the class token from the backbone output if necessary.\n",
    "        \"\"\"\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        \n",
    "        # Create a pretrained DeiT-Small model.\n",
    "        # Note: 'deit_small_patch16_224' is trained on 224x224 images.\n",
    "        # We will change the input resolution via transforms (to 400x400).\n",
    "        self.backbone = timm.create_model('deit_small_patch16_224', pretrained=True)\n",
    "        \n",
    "        # Freeze all parameters in the backbone.\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze the last two transformer blocks for fine-tuning.\n",
    "        # For DeiT, the transformer blocks are stored in self.backbone.blocks.\n",
    "        if hasattr(self.backbone, 'blocks'):\n",
    "            for block in self.backbone.blocks[-2:]:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = True\n",
    "        else:\n",
    "            print(\"Warning: Could not find transformer blocks in backbone.\")\n",
    "        \n",
    "        # Get the backbone's embedding dimension.\n",
    "        # For deit_small_patch16_224, the embedding dimension is stored in backbone.embed_dim.\n",
    "        backbone_embed_dim = self.backbone.embed_dim\n",
    "        \n",
    "        # Create an embedding layer mapping the backbone output to our desired embedding_dim.\n",
    "        self.embedding_layer = nn.Linear(backbone_embed_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "        - x: input tensor of shape [B, C, H, W], where H=W=400 (see transforms).\n",
    "        - The backbone returns features. For DeiT, it may return a tensor of shape [B, T, D],\n",
    "          where T is the number of tokens (e.g. 197 for 224x224 inputs).\n",
    "        - We select the class token (first token) so that the output is [B, D].\n",
    "        - Then we map through the embedding layer and L2-normalize the output.\n",
    "        \"\"\"\n",
    "        features = self.backbone.forward_features(x)\n",
    "        # Check if the output has a token dimension (i.e. 3 dimensions).\n",
    "        if features.ndim == 3:\n",
    "            # Select the class token (first token).\n",
    "            features = features[:, 0, :]  # Now shape becomes [B, D]\n",
    "        # Else, if features are already [B, D], do nothing.\n",
    "        embedding = self.embedding_layer(features)  # Shape: [B, embedding_dim]\n",
    "        # Normalize the embedding to unit length (beneficial for metric learning).\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae50c540-ec85-4ed3-bcb3-4f899bc47b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 2. Define the Prototypical Loss Function\n",
    "# =============================\n",
    "def prototypical_loss(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      - embeddings: Tensor of shape [B, embedding_dim] (one embedding per image).\n",
    "      - labels: Tensor of shape [B] (integer class labels, provided by ImageFolder).\n",
    "      \n",
    "    Steps:\n",
    "      1. Identify unique classes in the batch.\n",
    "      2. For each class, compute the prototype as the mean embedding of all samples in that class.\n",
    "      3. Remap the original labels to a batch-local index (0 to n_unique-1).\n",
    "      4. Compute the Euclidean distance between each sample's embedding and each prototype.\n",
    "      5. Use negative distances as logits (so that a smaller distance yields a higher score).\n",
    "      6. Compute cross-entropy loss with the remapped targets.\n",
    "      7. Also compute accuracy.\n",
    "    \"\"\"\n",
    "    # Ensure labels is a 1D tensor.\n",
    "    labels = labels.view(-1)\n",
    "    \n",
    "    # Get unique labels in the batch.\n",
    "    unique_labels = torch.unique(labels)\n",
    "    prototypes = []   # To hold the computed prototype for each class.\n",
    "    target_idxs = torch.empty_like(labels)\n",
    "    \n",
    "    # For each unique label, compute the prototype (mean embedding).\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = (labels == label)\n",
    "        # Calculate mean embedding for this class.\n",
    "        class_proto = embeddings[mask].mean(dim=0)\n",
    "        prototypes.append(class_proto)\n",
    "        # Map original label to a new index (0, 1, ..., n_unique-1).\n",
    "        target_idxs[mask] = i\n",
    "    prototypes = torch.stack(prototypes)  # Shape: [n_unique, embedding_dim]\n",
    "    \n",
    "    # Compute pairwise Euclidean distances.\n",
    "    # embeddings: [B, D] → unsqueeze to [B, 1, D]\n",
    "    # prototypes: [n_unique, D] → unsqueeze to [1, n_unique, D]\n",
    "    # The difference and squared sum yields a tensor of shape [B, n_unique].\n",
    "    dists = torch.sqrt(((embeddings.unsqueeze(1) - prototypes.unsqueeze(0)) ** 2).sum(dim=-1) + 1e-8)\n",
    "    \n",
    "    # Convert distances to logits by taking negative distances.\n",
    "    logits = -dists  # Shape: [B, n_unique]\n",
    "    \n",
    "    # Compute cross-entropy loss.\n",
    "    loss = F.cross_entropy(logits, target_idxs.long())\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = (preds == target_idxs).float().mean()\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4cfa26-da1d-4cea-b5cf-143e5bc11dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dir = \"G:/github_repos/tourist_sites_2/train\"\n",
    "test_dir  = \"G:/github_repos/tourist_sites_2/test\"\n",
    "\n",
    "# Create the datasets.\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset  = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f762263-7834-415e-8ab4-b2290347bb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\motaheda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\motaheda\\.cache\\huggingface\\hub\\models--timm--deit_small_patch16_224.fb_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 4. Instantiate Model and Optimizer\n",
    "# =============================\n",
    "model = PrototypicalNetwork(embedding_dim=128).to(device)\n",
    "\n",
    "# Only parameters with requires_grad=True (last two transformer blocks + embedding head) are updated.\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4632f639-53c6-4c98-ad3f-9c48a56f302c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                             | 1/15 [00:35<08:16, 35.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 1.5141, Train Acc: 0.9975 | Test Loss: 0.8984, Test Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████                                                                        | 2/15 [01:06<07:07, 32.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 | Train Loss: 1.4802, Train Acc: 0.9925 | Test Loss: 0.8256, Test Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 3/15 [01:38<06:27, 32.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 | Train Loss: 1.4400, Train Acc: 0.9962 | Test Loss: 0.7714, Test Acc: 0.9911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████▏                                                            | 4/15 [02:09<05:53, 32.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 | Train Loss: 1.3892, Train Acc: 0.9988 | Test Loss: 0.7403, Test Acc: 0.9821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▋                                                       | 5/15 [02:47<05:40, 34.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 | Train Loss: 1.3968, Train Acc: 0.9962 | Test Loss: 0.7337, Test Acc: 0.9821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 6/15 [03:21<05:07, 34.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 | Train Loss: 1.4022, Train Acc: 0.9975 | Test Loss: 0.7159, Test Acc: 0.9911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████████████████████▋                                            | 7/15 [03:52<04:24, 33.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 | Train Loss: 1.3594, Train Acc: 0.9975 | Test Loss: 0.6978, Test Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|████████████████████████████████████████████▎                                      | 8/15 [04:24<03:49, 32.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 | Train Loss: 1.3536, Train Acc: 0.9988 | Test Loss: 0.6901, Test Acc: 0.9911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 9/15 [04:58<03:19, 33.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 | Train Loss: 1.3501, Train Acc: 1.0000 | Test Loss: 0.6835, Test Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████▋                           | 10/15 [05:30<02:43, 32.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 | Train Loss: 1.3231, Train Acc: 1.0000 | Test Loss: 0.6704, Test Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████▏                     | 11/15 [06:02<02:09, 32.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 | Train Loss: 1.3646, Train Acc: 1.0000 | Test Loss: 0.6620, Test Acc: 0.9911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 12/15 [06:33<01:36, 32.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 | Train Loss: 1.3490, Train Acc: 0.9988 | Test Loss: 0.6593, Test Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████           | 13/15 [07:05<01:03, 31.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 | Train Loss: 1.3134, Train Acc: 0.9988 | Test Loss: 0.6578, Test Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████▌     | 14/15 [07:36<00:31, 31.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 | Train Loss: 1.3100, Train Acc: 1.0000 | Test Loss: 0.6539, Test Acc: 0.9911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [08:07<00:00, 32.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 | Train Loss: 1.3246, Train Acc: 1.0000 | Test Loss: 0.6480, Test Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 5. Training and Testing Loop\n",
    "# =============================\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    batch_count = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass: expect embeddings of shape [B, 128]\n",
    "        embeddings = model(images)\n",
    "        loss, acc = prototypical_loss(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    avg_train_loss = running_loss / batch_count\n",
    "    avg_train_acc = running_acc / batch_count\n",
    "    \n",
    "    # Evaluate on test set.\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            embeddings = model(images)\n",
    "            loss, acc = prototypical_loss(embeddings, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += acc.item()\n",
    "            test_batches += 1\n",
    "    avg_test_loss = test_loss / test_batches\n",
    "    avg_test_acc = test_acc / test_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f} | \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {avg_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d2e37c-ea28-4bbb-addb-07c16ff483f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 6. Compute and Save Final Class Prototypes\n",
    "# =============================\n",
    "# After training, compute a prototype (mean embedding) for each class using the entire training set.\n",
    "model.eval()\n",
    "class_prototypes = {}  # Dictionary: {class_index: prototype_tensor}\n",
    "num_classes = len(train_dataset.classes)\n",
    "# Dictionary to accumulate embeddings per class.\n",
    "all_embeddings = {i: [] for i in range(num_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74226e71-fde0-4dd0-9143-bb2772db0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        embeddings = model(images)  # Shape: [B, 128]\n",
    "        for emb, label in zip(embeddings, labels):\n",
    "            all_embeddings[label.item()].append(emb.cpu())\n",
    "            \n",
    "# Compute the mean embedding (prototype) for each class.\n",
    "for cls, emb_list in all_embeddings.items():\n",
    "    class_prototypes[cls] = torch.stack(emb_list).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba6038c1-1cf7-454d-b348-a6ce982c207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model and prototypes saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the full model.\n",
    "torch.save(model, \"full_prototypical_model.pth\")\n",
    "\n",
    "# Save the prototypes dictionary.\n",
    "torch.save(class_prototypes, \"class_prototypes.pth\")\n",
    "\n",
    "print(\"Full model and prototypes saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517109b-49c8-4cfe-a544-837bfb75181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [\n",
    "     'Abu Simbel Temple',\n",
    "     'Bibliotheca Alexandrina',\n",
    "     'Nefertari Temple',\n",
    "     'Saint Catherine Monastery',\n",
    "     'Citadel of Saladin',\n",
    "     'Monastery of St. Simeon',\n",
    "     'AlAzhar Mosque',\n",
    "     'Fortress of Shali in Siwa',\n",
    "     'Greek_Orthodox_Cemetery in Alexandria',\n",
    "     'Hanging Church',\n",
    "     'khan el khalili',\n",
    "     'Luxor Temple',\n",
    "     'Baron_empain',\n",
    "     'New Alamein City',\n",
    "     'Philae Temple',\n",
    "     'Pyramid of Djoser',\n",
    "     'Salt lake at Siwa',\n",
    "     'Wadi Al-Hitan',\n",
    "     'White Desert',\n",
    "     'Cairo Opera House',\n",
    "     'Tahrir Square',\n",
    "     'Cairo tower',\n",
    "     'Citadel of Qaitbay',\n",
    "     'Eg musuem',\n",
    "     'Great Pyramids of Giza',\n",
    "     'Hatshepsut temple',\n",
    "     'Meidum pyramid',\n",
    "     'Royal Montaza Palace'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1009544c-d715-4af4-bb79-90c9536fde98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 2-Baron_empain with distance 0.6200\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def predict_single_image(image_path, model, prototypes, transform, device, class_names=None):\n",
    "    \"\"\"\n",
    "    Predicts the class of a single image.\n",
    "    \n",
    "    Parameters:\n",
    "      - image_path: Path to the image file.\n",
    "      - model: Trained model that returns an embedding.\n",
    "      - prototypes: Dictionary {class_index: prototype_tensor}.\n",
    "      - transform: Transform pipeline used during training.\n",
    "      - device: 'cuda' or 'cpu'.\n",
    "      - class_names: (Optional) List mapping class indices to names.\n",
    "      \n",
    "    Returns:\n",
    "      - pred_class: Predicted class index.\n",
    "      - distances: Dictionary of L2 distances for each class.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image.\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)  # Shape: [1, C, H, W]\n",
    "    \n",
    "    # Extract embedding.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor)  # Shape: [1, embedding_dim]\n",
    "    \n",
    "    # Compute L2 distances to each prototype, ensuring the prototype is on the correct device.\n",
    "    distances = {cls: torch.norm(embedding - proto.to(device).unsqueeze(0)).item() \n",
    "                 for cls, proto in prototypes.items()}\n",
    "    \n",
    "    # Predict the class with the smallest distance.\n",
    "    pred_class = min(distances, key=distances.get)\n",
    "    \n",
    "    if class_names:\n",
    "        print(f\"Predicted class: {class_names[pred_class]} with distance {distances[pred_class]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Predicted class index: {pred_class} with distance {distances[pred_class]:.4f}\")\n",
    "    \n",
    "    return pred_class, distances\n",
    "\n",
    "#Use the prediction function for the provided image path.\n",
    "image_path = \"G:/github_repos/tourist_sites_2/22.jpg\"\n",
    "#pred_class, distances = predict_single_image(image_path, model, class_prototypes, transform, device, class_names=train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c40053-2922-4682-bf78-11dadecd0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the parameters of the model\n",
    "\n",
    "torch.save(model.state_dict(), \"model_state.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf29471-4ba2-43df-9bf9-75228bcdefab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
